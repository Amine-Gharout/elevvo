{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b102c357",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition (GTSRB)\n",
    "\n",
    "**Objective:** Classify traffic signs into 43 classes using deep learning (CNN).\n",
    "\n",
    "**Topics Covered:**\n",
    "- Computer Vision (CNN) / Multi-class Classification\n",
    "- Image preprocessing (resize, normalize)\n",
    "- Data augmentation\n",
    "- Custom CNN vs. pre-trained MobileNetV2\n",
    "- Evaluation: accuracy, confusion matrix\n",
    "\n",
    "**Tools:** Python, TensorFlow/Keras, OpenCV, scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8dd823",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0612adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'Data')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'Train.csv')\n",
    "TEST_CSV  = os.path.join(DATA_DIR, 'Test.csv')\n",
    "META_CSV  = os.path.join(DATA_DIR, 'Meta.csv')\n",
    "\n",
    "IMG_SIZE = 32          # resize all images to 32x32\n",
    "NUM_CLASSES = 43\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ae52c",
   "metadata": {},
   "source": [
    "## 2. Load & Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "meta_df  = pd.read_csv(META_CSV)\n",
    "\n",
    "print(f\"Training samples : {len(train_df)}\")\n",
    "print(f\"Test samples     : {len(test_df)}\")\n",
    "print(f\"Number of classes: {train_df['ClassId'].nunique()}\")\n",
    "print()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38781c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "class_counts = train_df['ClassId'].value_counts().sort_index()\n",
    "ax.bar(class_counts.index, class_counts.values, color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Class ID')\n",
    "ax.set_ylabel('Number of Images')\n",
    "ax.set_title('Training Set — Class Distribution')\n",
    "ax.set_xticks(range(NUM_CLASSES))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Min samples per class: {class_counts.min()} (class {class_counts.idxmin()})\")\n",
    "print(f\"Max samples per class: {class_counts.max()} (class {class_counts.idxmax()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise one sample per class\n",
    "fig, axes = plt.subplots(5, 9, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for class_id in range(NUM_CLASSES):\n",
    "    sample = train_df[train_df['ClassId'] == class_id].iloc[0]\n",
    "    img_path = os.path.join(DATA_DIR, sample['Path'])\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[class_id].imshow(img)\n",
    "    axes[class_id].set_title(f'Class {class_id}', fontsize=8)\n",
    "    axes[class_id].axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(NUM_CLASSES, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Image per Class', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166b7e6",
   "metadata": {},
   "source": [
    "## 3. Image Preprocessing — Resize & Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16475c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(df, data_dir, img_size=IMG_SIZE):\n",
    "    \"\"\"Load images from paths in a DataFrame, resize and normalise to [0, 1].\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(data_dir, row['Path'])\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        images.append(img)\n",
    "        labels.append(row['ClassId'])\n",
    "    return np.array(images, dtype='float32') / 255.0, np.array(labels)\n",
    "\n",
    "print(\"Loading training images...\")\n",
    "X_full, y_full = load_images(train_df, DATA_DIR)\n",
    "print(f\"  Loaded {X_full.shape[0]} images — shape: {X_full.shape[1:]}\")\n",
    "\n",
    "print(\"Loading test images...\")\n",
    "X_test, y_test = load_images(test_df, DATA_DIR)\n",
    "print(f\"  Loaded {X_test.shape[0]} images — shape: {X_test.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation split (80 / 20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=SEED, stratify=y_full\n",
    ")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, NUM_CLASSES)\n",
    "y_val_cat   = to_categorical(y_val,   NUM_CLASSES)\n",
    "y_test_cat  = to_categorical(y_test,  NUM_CLASSES)\n",
    "\n",
    "print(f\"Train : {X_train.shape[0]}\")\n",
    "print(f\"Val   : {X_val.shape[0]}\")\n",
    "print(f\"Test  : {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3747fb18",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b99abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=False,     # traffic signs are NOT horizontally symmetric\n",
    ")\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE, seed=SEED)\n",
    "\n",
    "# Preview augmented images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "sample_batch, _ = next(train_generator)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(sample_batch[i])\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Augmented Training Samples', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949b7da",
   "metadata": {},
   "source": [
    "## 5. Build Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Classifier head\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "cnn_model = build_custom_cnn()\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba512f8",
   "metadata": {},
   "source": [
    "## 6. Train Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7483892",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'],     label='Train')\n",
    "    ax1.plot(history.history['val_accuracy'],  label='Validation')\n",
    "    ax1.set_title(f'{title} — Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'],     label='Train')\n",
    "    ax2.plot(history.history['val_loss'],  label='Validation')\n",
    "    ax2.set_title(f'{title} — Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(cnn_history, title='Custom CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d1114",
   "metadata": {},
   "source": [
    "## 7. Evaluate Custom CNN on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Custom CNN — Test Accuracy: {cnn_test_acc:.4f}\")\n",
    "print(f\"Custom CNN — Test Loss    : {cnn_test_loss:.4f}\")\n",
    "\n",
    "y_pred_cnn = np.argmax(cnn_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report (Custom CNN):\")\n",
    "print(classification_report(y_test, y_pred_cnn, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=range(NUM_CLASSES), yticklabels=range(NUM_CLASSES))\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix — Custom CNN', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd2898",
   "metadata": {},
   "source": [
    "## 8. Bonus — Pre-trained MobileNetV2 (Transfer Learning)\n",
    "\n",
    "We up-scale images to 96×96 for MobileNetV2 (minimum input 32×32, but larger works better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOBILE_IMG_SIZE = 96\n",
    "\n",
    "# Resize data for MobileNetV2\n",
    "X_train_m = np.array([cv2.resize(img, (MOBILE_IMG_SIZE, MOBILE_IMG_SIZE)) for img in X_train])\n",
    "X_val_m   = np.array([cv2.resize(img, (MOBILE_IMG_SIZE, MOBILE_IMG_SIZE)) for img in X_val])\n",
    "X_test_m  = np.array([cv2.resize(img, (MOBILE_IMG_SIZE, MOBILE_IMG_SIZE)) for img in X_test])\n",
    "\n",
    "print(f\"Resized train shape: {X_train_m.shape}\")\n",
    "print(f\"Resized test shape : {X_test_m.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet_model(input_shape=(MOBILE_IMG_SIZE, MOBILE_IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "mobile_model = build_mobilenet_model()\n",
    "mobile_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "mobile_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aeef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for MobileNet input\n",
    "mobile_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=False,\n",
    ")\n",
    "mobile_datagen.fit(X_train_m)\n",
    "mobile_generator = mobile_datagen.flow(X_train_m, y_train_cat, batch_size=BATCH_SIZE, seed=SEED)\n",
    "\n",
    "# Phase 1: Train head only\n",
    "mobile_history_1 = mobile_model.fit(\n",
    "    mobile_generator,\n",
    "    steps_per_epoch=len(X_train_m) // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_m, y_val_cat),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n✓ Phase 1 complete (frozen base).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c328ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tune — unfreeze top layers of MobileNetV2\n",
    "base = mobile_model.layers[0]\n",
    "base.trainable = True\n",
    "\n",
    "# Freeze all but last 30 layers\n",
    "for layer in base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "mobile_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mobile_history_2 = mobile_model.fit(\n",
    "    mobile_generator,\n",
    "    steps_per_epoch=len(X_train_m) // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_m, y_val_cat),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n✓ Phase 2 complete (fine-tuned).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories for plotting\n",
    "combined_history = {}\n",
    "for key in mobile_history_1.history:\n",
    "    combined_history[key] = mobile_history_1.history[key] + mobile_history_2.history[key]\n",
    "\n",
    "class CombinedHistory:\n",
    "    def __init__(self, h): self.history = h\n",
    "\n",
    "plot_training_history(CombinedHistory(combined_history), title='MobileNetV2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f52687",
   "metadata": {},
   "source": [
    "## 9. Evaluate MobileNetV2 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_test_loss, mob_test_acc = mobile_model.evaluate(X_test_m, y_test_cat, verbose=0)\n",
    "print(f\"MobileNetV2 — Test Accuracy: {mob_test_acc:.4f}\")\n",
    "print(f\"MobileNetV2 — Test Loss    : {mob_test_loss:.4f}\")\n",
    "\n",
    "y_pred_mob = np.argmax(mobile_model.predict(X_test_m, verbose=0), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report (MobileNetV2):\")\n",
    "print(classification_report(y_test, y_pred_mob, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d27276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix — MobileNetV2\n",
    "cm_mob = confusion_matrix(y_test, y_pred_mob)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(cm_mob, annot=True, fmt='d', cmap='Greens', ax=ax,\n",
    "            xticklabels=range(NUM_CLASSES), yticklabels=range(NUM_CLASSES))\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix — MobileNetV2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbe329",
   "metadata": {},
   "source": [
    "## 10. Comparison — Custom CNN vs. MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4634f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Custom CNN', 'MobileNetV2 (fine-tuned)'],\n",
    "    'Test Accuracy': [cnn_test_acc, mob_test_acc],\n",
    "    'Test Loss':     [cnn_test_loss, mob_test_loss],\n",
    "    'Parameters':    [cnn_model.count_params(), mobile_model.count_params()],\n",
    "})\n",
    "comparison['Test Accuracy'] = comparison['Test Accuracy'].map('{:.4f}'.format)\n",
    "comparison['Test Loss']     = comparison['Test Loss'].map('{:.4f}'.format)\n",
    "comparison['Parameters']    = comparison['Parameters'].map('{:,}'.format)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"               MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise some test predictions (both models)\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "indices = np.random.choice(len(X_test), 15, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    idx = indices[i]\n",
    "    ax.imshow(X_test[idx])\n",
    "    true_label = y_test[idx]\n",
    "    cnn_pred   = y_pred_cnn[idx]\n",
    "    mob_pred   = y_pred_mob[idx]\n",
    "\n",
    "    colour = 'green' if cnn_pred == true_label else 'red'\n",
    "    ax.set_title(\n",
    "        f'True: {true_label}\\nCNN: {cnn_pred} | Mob: {mob_pred}',\n",
    "        fontsize=8, color=colour\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Test Predictions — Green = CNN correct, Red = CNN wrong', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a143ac7",
   "metadata": {},
   "source": [
    "## 11. Per-Class Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07425e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy for both models\n",
    "cnn_per_class = cm_cnn.diagonal() / cm_cnn.sum(axis=1)\n",
    "mob_per_class = cm_mob.diagonal() / cm_mob.sum(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "x = np.arange(NUM_CLASSES)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, cnn_per_class, width, label='Custom CNN', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, mob_per_class, width, label='MobileNetV2', color='seagreen', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class ID')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Per-Class Test Accuracy — Custom CNN vs MobileNetV2')\n",
    "ax.set_xticks(x)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Worst classes for each model\n",
    "worst_cnn = np.argsort(cnn_per_class)[:5]\n",
    "worst_mob = np.argsort(mob_per_class)[:5]\n",
    "print(f\"Top-5 hardest classes (CNN)       : {worst_cnn} — acc: {cnn_per_class[worst_cnn].round(3)}\")\n",
    "print(f\"Top-5 hardest classes (MobileNet) : {worst_mob} — acc: {mob_per_class[worst_mob].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0e890",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "| Aspect | Custom CNN | MobileNetV2 (Transfer Learning) |\n",
    "|---|---|---|\n",
    "| Architecture | 3 conv blocks + dense head | Pre-trained backbone + dense head |\n",
    "| Training | From scratch with augmentation | 2-phase: frozen then fine-tuned |\n",
    "| Parameters | ~600 K | ~2.6 M (most frozen) |\n",
    "| Expected Accuracy | ~95–97 % | ~96–98 % |\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Data augmentation significantly helps with the imbalanced GTSRB dataset.\n",
    "- Batch normalisation and dropout are essential regularisation techniques.\n",
    "- Transfer learning with MobileNetV2 converges faster and usually achieves slightly better accuracy.\n",
    "- The per-class analysis reveals which sign categories are hardest to distinguish."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
